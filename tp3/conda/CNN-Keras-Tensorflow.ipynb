{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing packages and the Keras libraries and we need to create CNN\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - The Sequential model is a linear stack of layers.\n",
    "model = Sequential()\n",
    "# Step 2 - Convolution2D creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. \n",
    "# If  use_bias is True, a bias vector is created and added to the outputs. \n",
    "# Finally, if activation is not None, it is applied to the outputs as well.\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape=(64, 64, 3)))\n",
    "# Step 3 - Max pooling is a sample-based discretization process. \n",
    "#The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), \n",
    "#reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "# Step 4 - Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, \n",
    "# which helps prevent overfitting.\n",
    "model.add(Dropout(0.3))\n",
    "# Step 5 - Convolution2D\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "# Step 6 - MaxPooling2D\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "# Step 7 - Dropout\n",
    "model.add(Dropout(0.3))\n",
    "# Step 8 - Flattens the input. Does not affect the batch size.\n",
    "model.add(Flatten())\n",
    "# Step 9 - Dense implements the operation: output = activation(dot(input, kernel) + bias) \n",
    "# where activation is the element-wise activation function passed as the activation argument, \n",
    "# kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer.\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "# Step 10 - Softmax is a generalization of the logistic function that \"squashes\" a K-dimensional vector Z\n",
    "# of arbitrary real values to a K-dimensional vector Sigma(z) of real values, \n",
    "# where each entry is in the range (0, 1), and all the entries add up to 1.\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 8,985,027\n",
      "Trainable params: 8,985,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 11 - Display summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 - Compile the trained model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use app from https://github.com/hardikvasa/google-images-download\n",
    "# Install -> $ pip install google_images_download\n",
    "#googleimagesdownload --keywords \"oak leaf\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"oakleaf\"\n",
    "#googleimagesdownload --keywords \"olive leaf -bottle, hoja de olivo\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"oliveleaf\"\n",
    "#googleimagesdownload --keywords \"salix leaf\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"salixleaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1009 images belonging to 3 classes.\n",
      "Found 185 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 13 - Generate datagens for training set and validation set\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, vertical_flip = True, horizontal_flip = True)\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('data/train', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', shuffle=True)\n",
    "validation_set = validation_datagen.flow_from_directory('data/validation', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', shuffle=True)\n",
    "\n",
    "clazz_map = (training_set.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14 - Setting the checkpoint that we will can return if we started again. \n",
    "model_name = \"cnn-tp3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n",
      "   7/1009 [..............................] - ETA: 7:47 - loss: 0.6336 - acc: 0.6667"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009/1009 [==============================] - 3675s 4s/step - loss: 0.5544 - acc: 0.7065 - val_loss: 0.4995 - val_acc: 0.7568\n",
      "Epoch 3/50\n",
      "1009/1009 [==============================] - 4307s 4s/step - loss: 0.4857 - acc: 0.7553 - val_loss: 0.4367 - val_acc: 0.8090\n",
      "Epoch 4/50\n",
      " 599/1009 [================>.............] - ETA: 43:03 - loss: 0.4240 - acc: 0.7919"
     ]
    }
   ],
   "source": [
    "# Step 15 - Train the model\n",
    "#model = load_model('cnn-tp3.hdf5')\n",
    "model_history = model.fit_generator(\n",
    "    training_set, \n",
    "    steps_per_epoch=1009, \n",
    "    initial_epoch=1, \n",
    "    epochs=50, \n",
    "    validation_data=validation_set, \n",
    "    validation_steps=180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16 - Save model\n",
    "model.save('cnn-tp3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 17 - Prediction\n",
    "prediction_image_names=['data/prediction/oakleaf1.jpg', \n",
    "                       'data/prediction/oakleaf2.jpg',\n",
    "                       'data/prediction/oliveleaf1.jpg',\n",
    "                       'data/prediction/oliveleaf2.jpg',\n",
    "                       'data/prediction/willowleaf1.jpg',\n",
    "                       'data/prediction/willowleaf2.jpg']\n",
    "\n",
    "for image_name in prediction_image_names:\n",
    "    test_image = image.load_img(image_name, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model.predict(test_image)\n",
    "    index = result.argmax(1)[0]\n",
    "    \n",
    "    for clazz in clazz_map.keys():\n",
    "        if clazz_map[clazz] == index:\n",
    "            print(\"The image belongs to the class: \", clazz)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
