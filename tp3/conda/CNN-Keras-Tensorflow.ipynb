{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing packages and the Keras libraries and we need to create CNN\n",
    "from keras.models import Sequential\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - The Sequential model is a linear stack of layers.\n",
    "model = Sequential()\n",
    "# Step 2 - Convolution2D creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. \n",
    "# If  use_bias is True, a bias vector is created and added to the outputs. \n",
    "# Finally, if activation is not None, it is applied to the outputs as well.\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu', input_shape=(64, 64, 3)))\n",
    "# Step 3 - Max pooling is a sample-based discretization process. \n",
    "#The objective is to down-sample an input representation (image, hidden-layer output matrix, etc.), \n",
    "#reducing its dimensionality and allowing for assumptions to be made about features contained in the sub-regions binned.\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "# Step 4 - Dropout consists in randomly setting a fraction rate of input units to 0 at each update during training time, \n",
    "# which helps prevent overfitting.\n",
    "model.add(Dropout(0.3))\n",
    "# Step 5 - Convolution2D\n",
    "model.add(Conv2D(filters=32, kernel_size=2, padding='same', activation='relu'))\n",
    "# Step 6 - MaxPooling2D\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "# Step 7 - Dropout\n",
    "model.add(Dropout(0.3))\n",
    "# Step 8 - Flattens the input. Does not affect the batch size.\n",
    "model.add(Flatten())\n",
    "# Step 9 - Dense implements the operation: output = activation(dot(input, kernel) + bias) \n",
    "# where activation is the element-wise activation function passed as the activation argument, \n",
    "# kernel is a weights matrix created by the layer, and bias is a bias vector created by the layer.\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.05))\n",
    "# Step 10 - Softmax is a generalization of the logistic function that \"squashes\" a K-dimensional vector Z\n",
    "# of arbitrary real values to a K-dimensional vector Sigma(z) of real values, \n",
    "# where each entry is in the range (0, 1), and all the entries add up to 1.\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 64, 64, 32)        416       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 32, 32, 32)        4128      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              8389632   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 8,985,027\n",
      "Trainable params: 8,985,027\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Step 11 - Display summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 12 - Compile the trained model\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use app from https://github.com/hardikvasa/google-images-download\n",
    "# Install -> $ pip install google_images_download\n",
    "#googleimagesdownload --keywords \"oak leaf\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"oakleaf\"\n",
    "#googleimagesdownload --keywords \"olive leaf -bottle, hoja de olivo\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"oliveleaf\"\n",
    "#googleimagesdownload --keywords \"salix leaf\" --no_numbering  --limit 50 --output_directory \"train\" --image_directory \"salixleaf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1009 images belonging to 3 classes.\n",
      "Found 185 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# Step 13 - Generate datagens for training set and validation set\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255, shear_range = 0.2, zoom_range = 0.2, vertical_flip = True, horizontal_flip = True)\n",
    "validation_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "training_set = train_datagen.flow_from_directory('data/train', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', shuffle=True)\n",
    "validation_set = validation_datagen.flow_from_directory('data/validation', target_size = (64, 64), batch_size = 32, class_mode = 'categorical', shuffle=True)\n",
    "\n",
    "clazz_map = (training_set.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 14 - Setting the checkpoint that we will can return if we started again. \n",
    "model_name = \"cnn-tp3.hdf5\"\n",
    "checkpoint = ModelCheckpoint(model_name, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "early_stopping = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=10, verbose=1, mode='auto')\n",
    "callbacks_list = [checkpoint, early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julian/anaconda3/lib/python3.6/site-packages/PIL/Image.py:916: UserWarning: Palette images with Transparency   expressed in bytes should be converted to RGBA images\n",
      "  'to RGBA images')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/31 [==============================] - 18s 550ms/step - loss: 0.4549 - acc: 0.7841 - val_loss: 0.4075 - val_acc: 0.8306\n",
      "Epoch 3/50\n",
      "32/31 [==============================] - 16s 491ms/step - loss: 0.4437 - acc: 0.7715 - val_loss: 0.3916 - val_acc: 0.8324\n",
      "Epoch 4/50\n",
      "32/31 [==============================] - 16s 490ms/step - loss: 0.4457 - acc: 0.7887 - val_loss: 0.4020 - val_acc: 0.8288\n",
      "Epoch 5/50\n",
      "32/31 [==============================] - 16s 487ms/step - loss: 0.4194 - acc: 0.8007 - val_loss: 0.3805 - val_acc: 0.8234\n",
      "Epoch 6/50\n",
      "32/31 [==============================] - 16s 494ms/step - loss: 0.4483 - acc: 0.7791 - val_loss: 0.3890 - val_acc: 0.8396\n",
      "Epoch 7/50\n",
      "32/31 [==============================] - 16s 490ms/step - loss: 0.4284 - acc: 0.7828 - val_loss: 0.3996 - val_acc: 0.8270\n",
      "Epoch 8/50\n",
      "32/31 [==============================] - 16s 495ms/step - loss: 0.4389 - acc: 0.7892 - val_loss: 0.3497 - val_acc: 0.8523\n",
      "Epoch 9/50\n",
      "32/31 [==============================] - 15s 483ms/step - loss: 0.4301 - acc: 0.7930 - val_loss: 0.4042 - val_acc: 0.8216\n",
      "Epoch 10/50\n",
      "32/31 [==============================] - 15s 483ms/step - loss: 0.4226 - acc: 0.7932 - val_loss: 0.3548 - val_acc: 0.8450\n",
      "Epoch 11/50\n",
      "32/31 [==============================] - 16s 489ms/step - loss: 0.4265 - acc: 0.7841 - val_loss: 0.3808 - val_acc: 0.8414\n",
      "Epoch 12/50\n",
      "32/31 [==============================] - 16s 486ms/step - loss: 0.4268 - acc: 0.7914 - val_loss: 0.3457 - val_acc: 0.8613\n",
      "Epoch 13/50\n",
      "32/31 [==============================] - 16s 492ms/step - loss: 0.4236 - acc: 0.7918 - val_loss: 0.3740 - val_acc: 0.8505\n",
      "Epoch 14/50\n",
      "32/31 [==============================] - 16s 493ms/step - loss: 0.4005 - acc: 0.8068 - val_loss: 0.3374 - val_acc: 0.8468\n",
      "Epoch 15/50\n",
      "32/31 [==============================] - 16s 493ms/step - loss: 0.4134 - acc: 0.7935 - val_loss: 0.3674 - val_acc: 0.8360\n",
      "Epoch 16/50\n",
      "32/31 [==============================] - 16s 490ms/step - loss: 0.4115 - acc: 0.8027 - val_loss: 0.3465 - val_acc: 0.8541\n",
      "Epoch 17/50\n",
      "32/31 [==============================] - 16s 488ms/step - loss: 0.3902 - acc: 0.8049 - val_loss: 0.3249 - val_acc: 0.8649\n",
      "Epoch 18/50\n",
      "32/31 [==============================] - 16s 494ms/step - loss: 0.4078 - acc: 0.8050 - val_loss: 0.3650 - val_acc: 0.8541\n",
      "Epoch 19/50\n",
      "32/31 [==============================] - 16s 488ms/step - loss: 0.3963 - acc: 0.8074 - val_loss: 0.3914 - val_acc: 0.8270\n",
      "Epoch 20/50\n",
      "32/31 [==============================] - 16s 490ms/step - loss: 0.3898 - acc: 0.8148 - val_loss: 0.2957 - val_acc: 0.8775\n",
      "Epoch 21/50\n",
      "32/31 [==============================] - 16s 492ms/step - loss: 0.4069 - acc: 0.7965 - val_loss: 0.3505 - val_acc: 0.8505\n",
      "Epoch 22/50\n",
      "32/31 [==============================] - 18s 576ms/step - loss: 0.3765 - acc: 0.8177 - val_loss: 0.3597 - val_acc: 0.8486\n",
      "Epoch 23/50\n",
      "32/31 [==============================] - 16s 494ms/step - loss: 0.3902 - acc: 0.8179 - val_loss: 0.3319 - val_acc: 0.8541\n",
      "Epoch 24/50\n",
      "32/31 [==============================] - 16s 491ms/step - loss: 0.3812 - acc: 0.8182 - val_loss: 0.3218 - val_acc: 0.8685\n",
      "Epoch 25/50\n",
      "32/31 [==============================] - 16s 492ms/step - loss: 0.3595 - acc: 0.8233 - val_loss: 0.3334 - val_acc: 0.8613\n",
      "Epoch 26/50\n",
      "32/31 [==============================] - 16s 493ms/step - loss: 0.3876 - acc: 0.8057 - val_loss: 0.3554 - val_acc: 0.8505\n",
      "Epoch 27/50\n",
      "32/31 [==============================] - 16s 486ms/step - loss: 0.3896 - acc: 0.8063 - val_loss: 0.3134 - val_acc: 0.8757\n",
      "Epoch 28/50\n",
      "32/31 [==============================] - 16s 490ms/step - loss: 0.3671 - acc: 0.8251 - val_loss: 0.3176 - val_acc: 0.8739\n",
      "Epoch 29/50\n",
      "32/31 [==============================] - 16s 491ms/step - loss: 0.3797 - acc: 0.8123 - val_loss: 0.3132 - val_acc: 0.8757\n",
      "Epoch 30/50\n",
      "32/31 [==============================] - 16s 495ms/step - loss: 0.3726 - acc: 0.8205 - val_loss: 0.3333 - val_acc: 0.8703\n",
      "Epoch 31/50\n",
      "32/31 [==============================] - 16s 496ms/step - loss: 0.3562 - acc: 0.8240 - val_loss: 0.2923 - val_acc: 0.8829\n",
      "Epoch 32/50\n",
      "32/31 [==============================] - 6319s 197s/step - loss: 0.3538 - acc: 0.8236 - val_loss: 0.3163 - val_acc: 0.8667\n",
      "Epoch 33/50\n",
      "32/31 [==============================] - 27s 837ms/step - loss: 0.3735 - acc: 0.8276 - val_loss: 0.3198 - val_acc: 0.8721\n",
      "Epoch 34/50\n",
      "32/31 [==============================] - 4477s 140s/step - loss: 0.3471 - acc: 0.8401 - val_loss: 0.2973 - val_acc: 0.8865\n",
      "Epoch 35/50\n",
      "32/31 [==============================] - 9476s 296s/step - loss: 0.3636 - acc: 0.8285 - val_loss: 0.2813 - val_acc: 0.8793\n",
      "Epoch 36/50\n",
      "32/31 [==============================] - 19s 580ms/step - loss: 0.3440 - acc: 0.8378 - val_loss: 0.3039 - val_acc: 0.8631\n",
      "Epoch 37/50\n",
      "32/31 [==============================] - 16s 497ms/step - loss: 0.3358 - acc: 0.8390 - val_loss: 0.2806 - val_acc: 0.8901\n",
      "Epoch 38/50\n",
      "32/31 [==============================] - 16s 493ms/step - loss: 0.3295 - acc: 0.8441 - val_loss: 0.3039 - val_acc: 0.8757\n",
      "Epoch 39/50\n",
      "32/31 [==============================] - 16s 503ms/step - loss: 0.3399 - acc: 0.8367 - val_loss: 0.2815 - val_acc: 0.8847\n",
      "Epoch 40/50\n",
      "32/31 [==============================] - 16s 501ms/step - loss: 0.3328 - acc: 0.8476 - val_loss: 0.2705 - val_acc: 0.8811\n",
      "Epoch 41/50\n",
      "32/31 [==============================] - 16s 512ms/step - loss: 0.3162 - acc: 0.8468 - val_loss: 0.2965 - val_acc: 0.8739\n",
      "Epoch 42/50\n",
      "32/31 [==============================] - 17s 518ms/step - loss: 0.3409 - acc: 0.8478 - val_loss: 0.2592 - val_acc: 0.9081\n",
      "Epoch 43/50\n",
      "32/31 [==============================] - 16s 497ms/step - loss: 0.3238 - acc: 0.8435 - val_loss: 0.2623 - val_acc: 0.9045\n",
      "Epoch 44/50\n",
      "32/31 [==============================] - 16s 495ms/step - loss: 0.3400 - acc: 0.8518 - val_loss: 0.2850 - val_acc: 0.8937\n",
      "Epoch 45/50\n",
      "32/31 [==============================] - 16s 491ms/step - loss: 0.3166 - acc: 0.8482 - val_loss: 0.2760 - val_acc: 0.8757\n",
      "Epoch 46/50\n",
      "32/31 [==============================] - 16s 497ms/step - loss: 0.3046 - acc: 0.8497 - val_loss: 0.2424 - val_acc: 0.8919\n",
      "Epoch 47/50\n",
      "32/31 [==============================] - 16s 494ms/step - loss: 0.3280 - acc: 0.8543 - val_loss: 0.2722 - val_acc: 0.8883\n",
      "Epoch 48/50\n",
      "32/31 [==============================] - 16s 486ms/step - loss: 0.3331 - acc: 0.8372 - val_loss: 0.2588 - val_acc: 0.8847\n",
      "Epoch 49/50\n",
      "32/31 [==============================] - 16s 491ms/step - loss: 0.3132 - acc: 0.8514 - val_loss: 0.2637 - val_acc: 0.8829\n",
      "Epoch 50/50\n",
      "32/31 [==============================] - 16s 504ms/step - loss: 0.2883 - acc: 0.8655 - val_loss: 0.2950 - val_acc: 0.8865\n"
     ]
    }
   ],
   "source": [
    "# Step 15 - Train the model\n",
    "model = load_model('cnn-tp3.hdf5')\n",
    "model_history = model.fit_generator(\n",
    "    training_set, \n",
    "    steps_per_epoch=1009/32, \n",
    "    initial_epoch=1, \n",
    "    epochs=50, \n",
    "    validation_data=validation_set, \n",
    "    validation_steps=185/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 16 - Save model\n",
    "model.save('cnn-tp3.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image belongs to the class:  oakleaf\n",
      "The image belongs to the class:  oakleaf\n",
      "The image belongs to the class:  oliveleaf\n",
      "The image belongs to the class:  oliveleaf\n",
      "The image belongs to the class:  oakleaf\n",
      "The image belongs to the class:  oakleaf\n"
     ]
    }
   ],
   "source": [
    "# Step 17 - Prediction\n",
    "prediction_image_names=['data/prediction/oakleaf1.jpg', \n",
    "                       'data/prediction/oakleaf2.jpg',\n",
    "                       'data/prediction/oliveleaf1.jpg',\n",
    "                       'data/prediction/oliveleaf2.jpg',\n",
    "                       'data/prediction/willowleaf1.jpg',\n",
    "                       'data/prediction/willowleaf2.jpg']\n",
    "\n",
    "for image_name in prediction_image_names:\n",
    "    test_image = image.load_img(image_name, target_size = (64, 64))\n",
    "    test_image = image.img_to_array(test_image)\n",
    "    test_image = np.expand_dims(test_image, axis = 0)\n",
    "    result = model.predict(test_image)\n",
    "    index = result.argmax(1)[0]\n",
    "    \n",
    "    for clazz in clazz_map.keys():\n",
    "        if clazz_map[clazz] == index:\n",
    "            print(\"The image belongs to the class: \", clazz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
